name: AI Evaluation Test Suite

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'services/**'
      - 'evaluations/**'
      - 'fastapi_backend.py'
  pull_request:
    branches: [ main ]
    paths:
      - 'services/**'
      - 'evaluations/**'
      - 'fastapi_backend.py'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  ai-evaluation-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
        test-suite: [unit, integration, e2e, performance]
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-benchmark
        
    - name: Set up test environment
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_AI_API_KEY: ${{ secrets.GOOGLE_AI_API_KEY }}
      run: |
        export PYTHONPATH="${PYTHONPATH}:${GITHUB_WORKSPACE}"
        mkdir -p test_reports
        
    - name: Run Unit Tests
      if: matrix.test-suite == 'unit'
      run: |
        cd evaluations
        python -m pytest test_financial_agent.py test_marketing_agent.py \
          --cov=. --cov-report=xml --cov-report=html \
          --junitxml=test_reports/unit_tests.xml -v
          
    - name: Run Integration Tests
      if: matrix.test-suite == 'integration'
      run: |
        cd evaluations
        python -m pytest test_model_integration.py test_business_intelligence.py \
          --cov=. --cov-report=xml --cov-report=html \
          --junitxml=test_reports/integration_tests.xml -v
          
    - name: Run E2E Tests
      if: matrix.test-suite == 'e2e'
      run: |
        cd evaluations
        python -m pytest test_conversation_quality.py test_ai_agent_evaluation_suite.py \
          --cov=. --cov-report=xml --cov-report=html \
          --junitxml=test_reports/e2e_tests.xml -v
          
    - name: Run Performance Tests
      if: matrix.test-suite == 'performance'
      run: |
        cd evaluations
        python -m pytest test_performance_benchmarks.py test_safety_reliability.py \
          --benchmark-only --benchmark-json=test_reports/benchmarks.json \
          --junitxml=test_reports/performance_tests.xml -v
          
    - name: Generate Comprehensive Report
      if: matrix.test-suite == 'unit' && matrix.python-version == '3.11'
      run: |
        cd evaluations
        python run_comprehensive_tests.py --report-only
        
    - name: Upload Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.test-suite }}
        path: |
          evaluations/test_reports/
          evaluations/htmlcov/
          
    - name: Upload Coverage to Codecov
      if: matrix.test-suite == 'unit' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./evaluations/coverage.xml
        flags: ai-evaluation-tests
        name: codecov-ai-evaluation
        
    - name: Performance Regression Check
      if: matrix.test-suite == 'performance'
      run: |
        cd evaluations
        python -c "
        import json
        with open('test_reports/benchmarks.json') as f:
            benchmarks = json.load(f)
        
        # Check if any benchmark exceeds thresholds
        failures = []
        for benchmark in benchmarks['benchmarks']:
            if 'response_time' in benchmark['name']:
                if benchmark['stats']['mean'] > 5.0:  # 5 second threshold
                    failures.append(f'{benchmark[\"name\"]}: {benchmark[\"stats\"][\"mean\"]:.2f}s')
                    
        if failures:
            print('Performance regression detected:')
            for failure in failures:
                print(f'  {failure}')
            exit(1)
        else:
            print('All performance benchmarks passed')
        "
        
  security-scan:
    runs-on: ubuntu-latest
    needs: ai-evaluation-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Safety Tests
      run: |
        cd evaluations
        python -m pytest test_safety_reliability.py::TestDataPrivacyProtection \
          --junitxml=test_reports/security_tests.xml -v
          
    - name: Security Vulnerability Scan
      uses: pypa/gh-action-pip-audit@v1.0.8
      with:
        inputs: requirements.txt
        
  deployment-readiness:
    runs-on: ubuntu-latest
    needs: [ai-evaluation-tests, security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Validate Model Performance
      run: |
        cd evaluations
        python run_comprehensive_tests.py --quick
        
        # Check if success criteria are met
        python -c "
        import json
        with open('test_reports/test_results.json') as f:
            results = json.load(f)
            
        # Validate success criteria
        overall_score = results.get('aggregate_metrics', {}).get('overall_score', 0)
        if overall_score < 0.85:  # 85% threshold
            print(f'Deployment blocked: Overall score {overall_score:.2f} below 0.85 threshold')
            exit(1)
        else:
            print(f'Deployment approved: Overall score {overall_score:.2f}')
        "
        
    - name: Generate Deployment Report
      run: |
        cd evaluations
        echo "## AI System Evaluation Report" > deployment_report.md
        echo "**Date**: $(date)" >> deployment_report.md
        echo "**Commit**: ${{ github.sha }}" >> deployment_report.md
        echo "" >> deployment_report.md
        
        python -c "
        import json
        with open('test_reports/test_results.json') as f:
            results = json.load(f)
            
        metrics = results.get('aggregate_metrics', {})
        print(f'**Overall Score**: {metrics.get(\"overall_score\", 0):.2f}')
        print(f'**Test Coverage**: {metrics.get(\"test_coverage\", 0):.2f}%')
        print(f'**Performance Score**: {metrics.get(\"performance_score\", 0):.2f}')
        print(f'**Safety Score**: {metrics.get(\"safety_score\", 0):.2f}')
        " >> deployment_report.md
        
    - name: Create Release
      if: success()
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ai-eval-${{ github.run_number }}
        release_name: AI Evaluation Release ${{ github.run_number }}
        body_path: evaluations/deployment_report.md
        draft: false
        prerelease: false